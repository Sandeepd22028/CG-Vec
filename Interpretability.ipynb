{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2043a211",
   "metadata": {},
   "source": [
    "CG construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e12c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, functools, json, warnings, re\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pymatgen.core import Element, Structure, Lattice\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "class GaussianDistance(object):\n",
    "    def __init__(self, dmin, dmax, step, var=None):\n",
    "        assert dmin < dmax\n",
    "        assert dmax - dmin > step\n",
    "        self.filter = np.arange(dmin, dmax + step, step)\n",
    "        self.var = step if var is None else var\n",
    "\n",
    "    def expand(self, distances):\n",
    "        return np.exp(-(distances[..., np.newaxis] - self.filter) ** 2 / self.var ** 2)\n",
    "\n",
    "\n",
    "class CIFData(Dataset):\n",
    "    def __init__(self, id_prop_path, magmom_path, eigen_path, ofm_val_comp_path,\n",
    "             max_num_nbr=12, radius=8, dmin=0, step=0.1, random_seed=123):\n",
    "\n",
    "\n",
    "        # Load CSV\n",
    "        with open(id_prop_path) as f:\n",
    "            reader = csv.reader(f)\n",
    "            self.id_prop_data = [row for row in reader]\n",
    "\n",
    "        # Load magnetic moments\n",
    "        with open(magmom_path) as f:\n",
    "            self.magmom = json.load(f)\n",
    "\n",
    "        # Load magnetic moments\n",
    "        with open(eigen_path) as f:\n",
    "            self.eigen1 = json.load(f)\n",
    "\n",
    "        # Load magnetic moments\n",
    "        with open(ofm_val_comp_path) as f:\n",
    "            self.ofm_val_comp = json.load(f)\n",
    "\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(self.id_prop_data)\n",
    "\n",
    "        self.gdf = GaussianDistance(dmin=dmin, dmax=radius, step=step)\n",
    "        self.max_num_nbr = max_num_nbr\n",
    "        self.radius = radius\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_prop_data)\n",
    "\n",
    "    def structure_from_str(self, cif_string):\n",
    "        cif_lines = cif_string.strip().split('\\n')\n",
    "\n",
    "    # Parse lattice\n",
    "        a, b, c = map(float, cif_lines[2].split()[2:5])\n",
    "        alpha, beta, gamma = map(float, cif_lines[3].split()[1:4])\n",
    "\n",
    "        species, coords = [], []\n",
    "\n",
    "        reading_atoms = False\n",
    "        for line in cif_lines:\n",
    "            line = line.strip()\n",
    "            if re.match(r'^\\d+\\s+\\w+\\s+[\\d\\.Ee+-]+\\s+[\\d\\.Ee+-]+\\s+[\\d\\.Ee+-]+', line):\n",
    "                reading_atoms = True\n",
    "            if not reading_atoms:\n",
    "                continue\n",
    "            try:\n",
    "                parts = line.split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                sp = parts[1]\n",
    "                a_frac, b_frac, c_frac = map(float, parts[2:5])\n",
    "                species.append(sp)\n",
    "                coords.append([a_frac, b_frac, c_frac])\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Skipping line due to error: {line} ({e})\")\n",
    "                continue\n",
    "\n",
    "        lattice = Lattice.from_parameters(a, b, c, alpha, beta, gamma)\n",
    "        return Structure(lattice, species, coords)\n",
    "\n",
    "\n",
    "    def featurizer(self, element):\n",
    "        min_max = [[1.00794, 244.0], [1.0, 118.0], [0.25, 2.6], [-0.72, 3.61272528]]\n",
    "        lis = ['Atomic mass', 'Atomic no', 'Atomic radius', 'Electron affinity']\n",
    "        ele = Element(element)\n",
    "        try:\n",
    "            fea = [float(str(ele.data[i]).split()[0]) for i in lis]\n",
    "        except:\n",
    "            fea = [ele.data['Atomic mass'], ele.data['Atomic no'],\n",
    "                   ele.data['Atomic radius calculated'], ele.data['Electron affinity']]\n",
    "        for i in range(4):\n",
    "            fea[i] = (fea[i] - min_max[i][0]) / (min_max[i][1] - min_max[i][0])\n",
    "        return fea\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        cif_id, cif_string, target = self.id_prop_data[idx]\n",
    "        crystal = self.structure_from_str(cif_string)\n",
    "\n",
    "        atom_fea = np.vstack([\n",
    "            [self.magmom[site.species_string]] + self.ofm_val_comp[site.species_string]\n",
    "            + self.featurizer(site.species_string)\n",
    "            for site in crystal\n",
    "        ])\n",
    "        atom_fea = torch.Tensor(atom_fea)\n",
    "\n",
    "        all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)\n",
    "        all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n",
    "        nbr_fea_idx, nbr_fea = [], []\n",
    "\n",
    "        for i, nbr in enumerate(all_nbrs):\n",
    "            if len(nbr) < self.max_num_nbr:\n",
    "                warnings.warn(f'{cif_id} atom {i} has only {len(nbr)} neighbors. Consider increasing radius.')\n",
    "                nbr_fea_idx.append([x[2] for x in nbr] + [0] * (self.max_num_nbr - len(nbr)))\n",
    "                nbr_fea.append([x[1] for x in nbr] + [self.radius + 1.] * (self.max_num_nbr - len(nbr)))\n",
    "            else:\n",
    "                nbr_fea_idx.append([x[2] for x in nbr[:self.max_num_nbr]])\n",
    "                nbr_fea.append([x[1] for x in nbr[:self.max_num_nbr]])\n",
    "\n",
    "        nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n",
    "        nbr_fea = torch.Tensor(self.gdf.expand(np.array(nbr_fea)))\n",
    "        target = torch.Tensor([float(target)])\n",
    "        return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9f4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 3000 entries.\n",
      "First CSV row preview: ['mp-1219989', 'Full Formula (Pr1 Fe1 Co4)\\nReduced Formula: PrFeCo4\\nabc   :   5.070117   5.070117   3.941409\\nangles:  90.000000  90.000000 120.000008\\npbc   :       True       True       True\\nSites (6)\\n  #  SP           a         b    c    magmom\\n---  ----  --------  --------  ---  --------\\n  0  Pr    0.666667  0.333333  0      -0.236\\n  1  Fe    0         0         0       2.604\\n  2  Co    0.333333  0.666667  0       1.615\\n  3  Co    0.167998  0.335996  0.5     1.564\\n  4  Co    0.664004  0.832002  0.5     1.342\\n  5  Co    0.167998  0.832002  0.5     1.598']\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate dataset with exact file paths â€” edit these paths if your files are elsewhere.\n",
    "dataset = CIFData(\n",
    "    id_prop_path='my_new_data1/all_fim_3000.csv',                 # <-- your CSV of (id, cif_string, target)\n",
    "    magmom_path='my_new_data1/magmom.json',             # <-- edit if located elsewhere\n",
    "    eigen_path='my_new_data1/eigenvalues_bulk.json',    # <-- edit if located elsewhere\n",
    "    ofm_val_comp_path='my_new_data1/ofm.json',          # <-- edit if located elsewhere\n",
    "    max_num_nbr=12,\n",
    "    radius=8,\n",
    "    dmin=0,\n",
    "    step=0.1,\n",
    "    random_seed=123\n",
    ")\n",
    "\n",
    "# Quick sanity prints\n",
    "print(\"Loaded dataset with\", len(dataset), \"entries.\")\n",
    "print(\"First CSV row preview:\", dataset.id_prop_data[0][:2])  # id and first part of cif string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6a35c",
   "metadata": {},
   "source": [
    "CG-Vec Dimension expansion for tracing feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ed29a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_node= 37 num_rbf= 81 inferred D_edge= 81\n",
      "D_total= 6386 mapping entries= 165\n",
      "Saved mapping_global.json\n",
      "  vectorized 500/3000\n",
      "  vectorized 1000/3000\n",
      "  vectorized 1500/3000\n",
      "  vectorized 2000/3000\n",
      "  vectorized 2500/3000\n",
      "  vectorized 3000/3000\n",
      "Vectorization time (s): 45.8219211101532\n",
      "Saved X,y and empty per_sample_topk_records (top-K not used).\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Cell B (minimal): CG -> VEC vectorization using only original features\n",
    "# - Species blocks: count + mean(node_features) + std(node_features)\n",
    "# - Edge RBF summaries: mean & std per RBF bin across all edges\n",
    "# - Global mean & std per node_feature\n",
    "# No top-K slots, no compression beyond RBF bins.\n",
    "# ---------------------------\n",
    "\n",
    "import os, json, time, numpy as np, torch\n",
    "\n",
    "# PARAMETERS (do not change unless you understand consequences)\n",
    "id_prop_list = dataset.id_prop_data\n",
    "RBF_step = 0.1   #  this matches your CIFData GaussianDistance.step\n",
    "# We will use the RBF centers as constructed by GaussianDistance in dataset.gdf.filter\n",
    "rbf_centers = dataset.gdf.filter\n",
    "num_rbf = len(rbf_centers)  # number of gaussian bins (e.g., (radius - dmin)/step + 1 )\n",
    "\n",
    "os.makedirs('cgvec_mapping', exist_ok=True)\n",
    "\n",
    "# 1) global species set\n",
    "global_species_set = set()\n",
    "for row in id_prop_list:\n",
    "    cif_id, cif_string, _ = row\n",
    "    struct = dataset.structure_from_str(cif_string)\n",
    "    for site in struct:\n",
    "        global_species_set.add(site.species_string)\n",
    "global_species = sorted(list(global_species_set))\n",
    "n_species = len(global_species)\n",
    "\n",
    "# 2) infer node dimension and rbf bins\n",
    "(first_atom_fea, first_nbr_fea, first_nbr_idx), _, _ = dataset[0]\n",
    "D_node = int(first_atom_fea.shape[1])   # should be 37\n",
    "D_edge = int(first_nbr_fea.shape[2]) if (first_nbr_fea is not None and first_nbr_fea.numel()>0) else 0\n",
    "print(\"D_node=\", D_node, \"num_rbf=\", num_rbf, \"inferred D_edge=\", D_edge)\n",
    "\n",
    "# 3) build mapping_global deterministically\n",
    "mapping_global = []\n",
    "cursor = 0\n",
    "# species blocks\n",
    "for s in global_species:\n",
    "    block_len = 1 + D_node + D_node\n",
    "    mapping_global.append({'type':'species_block','species':s,'start':cursor,'end':cursor+block_len})\n",
    "    cursor += block_len\n",
    "# edge RBF bins: each RBF bin -> mean & std (2 scalars)\n",
    "for k in range(num_rbf):\n",
    "    mapping_global.append({'type':'edge_rbf_bin','rbf_idx':int(k),'start':cursor,'end':cursor+2})\n",
    "    cursor += 2\n",
    "# global mean & std\n",
    "mapping_global.append({'type':'global_atom_mean','start':cursor,'end':cursor + D_node}); cursor += D_node\n",
    "mapping_global.append({'type':'global_atom_std','start':cursor,'end':cursor + D_node}); cursor += D_node\n",
    "\n",
    "D_total = cursor\n",
    "print(\"D_total=\", D_total, \"mapping entries=\", len(mapping_global))\n",
    "\n",
    "with open('cgvec_mapping/mapping_global.json','w') as f:\n",
    "    json.dump(mapping_global, f, indent=2)\n",
    "print(\"Saved mapping_global.json\")\n",
    "\n",
    "# 4) vectorize dataset (species blocks + rbf bin summaries + global stats)\n",
    "N = len(dataset)\n",
    "X = np.zeros((N, D_total), dtype=np.float32)\n",
    "y = np.zeros(N, dtype=float)\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(N):\n",
    "    (atom_fea, nbr_fea, nbr_idx), target, cif_id = dataset[i]\n",
    "    struct = dataset.structure_from_str(dataset.id_prop_data[i][1])\n",
    "    species_list = [site.species_string for site in struct]\n",
    "    atom = atom_fea.detach().cpu().numpy()  # shape (n_atoms, D_node)\n",
    "\n",
    "    parts = []\n",
    "    # species blocks\n",
    "    for s in global_species:\n",
    "        idxs = [j for j,sp in enumerate(species_list) if sp==s]\n",
    "        if len(idxs)==0:\n",
    "            cnt = 0.0\n",
    "            mean = np.zeros(D_node, dtype=float)\n",
    "            std  = np.zeros(D_node, dtype=float)\n",
    "        else:\n",
    "            feats = atom[idxs]\n",
    "            cnt = float(feats.shape[0])\n",
    "            mean = feats.mean(axis=0)\n",
    "            std  = feats.std(axis=0, ddof=0)\n",
    "        parts.append(np.concatenate([[cnt], mean, std]).astype(float))\n",
    "\n",
    "    # edge RBF bin summaries (mean & std per RBF bin across all edges)\n",
    "    if nbr_fea is None or (isinstance(nbr_fea, torch.Tensor) and nbr_fea.numel()==0):\n",
    "        # if no neighbor data, pad zeros for each bin\n",
    "        for _ in range(num_rbf):\n",
    "            parts.append(np.array([0.0, 0.0], dtype=float))\n",
    "    else:\n",
    "        flat = nbr_fea.detach().cpu().numpy().reshape(-1, nbr_fea.shape[2])  # shape (n_edges, num_rbf)\n",
    "        if flat.size == 0:\n",
    "            for _ in range(num_rbf):\n",
    "                parts.append(np.array([0.0, 0.0], dtype=float))\n",
    "        else:\n",
    "            # per RBF bin compute mean & std across edges\n",
    "            for k in range(num_rbf):\n",
    "                col = flat[:, k]\n",
    "                parts.append(np.array([float(col.mean()), float(col.std(ddof=0))], dtype=float))\n",
    "\n",
    "    # global mean & std\n",
    "    g_mean = atom.mean(axis=0) if atom.size else np.zeros(D_node, dtype=float)\n",
    "    g_std  = atom.std(axis=0, ddof=0) if atom.size else np.zeros(D_node, dtype=float)\n",
    "    parts.append(g_mean); parts.append(g_std)\n",
    "\n",
    "    vec = np.concatenate(parts).astype(np.float32)\n",
    "    if vec.size != D_total:\n",
    "        raise ValueError(f\"vec size mismatch {vec.size} vs D_total {D_total} at sample {i}\")\n",
    "    X[i,:] = vec\n",
    "    y[i] = float(target)\n",
    "\n",
    "    if (i+1) % 500 == 0 or (i+1) == N:\n",
    "        print(f\"  vectorized {i+1}/{N}\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Vectorization time (s):\", t1-t0)\n",
    "\n",
    "# 5) Save X,y and mapping\n",
    "np.save('cgvec_mapping/X_cgvecpp.npy', X)\n",
    "np.save('cgvec_mapping/y_cgvecpp.npy', y)\n",
    "with open('cgvec_mapping/per_sample_topk_records.json','w') as f:\n",
    "    json.dump([], f)  # empty list since we did not compute top-K slots in this minimal run\n",
    "print(\"Saved X,y and empty per_sample_topk_records (top-K not used).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075c95c",
   "metadata": {},
   "source": [
    "Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13baadb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sand\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (3000, 6386), y: (3000,)\n",
      "Inferred D_node=37, rbf_bins=81\n",
      "Saved feature_index_meaning.csv\n",
      "Using 10 quantile bins for stratified split (min bin size 300)\n",
      "Split sizes -> train: 1800 val: 600 test: 600\n",
      "RF trained in 6.32s and saved to cgvec_mapping\\rf_cgvecpp.joblib\n",
      "Validation metrics: {'R2': 0.8290945465844328, 'MAE': 0.010229092457254106, 'RMSE': 0.015034421346184676, 'CC': 0.9110879481778826}\n",
      "Test metrics: {'R2': 0.786030639499318, 'MAE': 0.01115738538253742, 'RMSE': 0.017026803451910343, 'CC': 0.8867419508969241}\n",
      "Saved feature_importances_vector_index.csv\n",
      "Saved importance_by_meaning_impurity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sand\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sand\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHAP values with TreeExplainer(feature_perturbation='interventional') on eval subset (size 500)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 496/500 [00:29<00:00]        C:\\Users\\Sand\\AppData\\Local\\Temp\\ipykernel_3756\\3228824978.py:194: UserWarning: Interventional explainer failed with: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.047919, while the model output was 0.049662. If this difference is acceptable you can set check_additivity=False to disable this check.. Falling back to check_additivity=False with default perturbation.\n",
      "  warnings.warn(f\"Interventional explainer failed with: {e}. Falling back to check_additivity=False with default perturbation.\")\n",
      " 97%|=================== | 486/500 [00:29<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature_importances_with_shap.csv (robust SHAP)\n",
      "Saved importance_by_meaning_shap.csv (robust SHAP)\n",
      "\n",
      "Top-20 SHAP-aggregated meanings (robust):\n",
      "                            meaning  mean_abs_shap\n",
      "      global_mean__OFM_component_13       0.017346\n",
      "                global_mean__magmom       0.006488\n",
      "      global_mean__OFM_component_14       0.002612\n",
      "                    rbf_bin25__mean       0.002054\n",
      "       global_std__OFM_component_13       0.001367\n",
      "       global_std__OFM_component_12       0.001043\n",
      "                    rbf_bin20__mean       0.000962\n",
      "      global_mean__OFM_component_12       0.000958\n",
      "       global_std__OFM_component_24       0.000817\n",
      "global_mean__electron_affinity_norm       0.000730\n",
      "                    rbf_bin19__mean       0.000725\n",
      "                          Fe__count       0.000643\n",
      "                     rbf_bin19__std       0.000585\n",
      " global_std__electron_affinity_norm       0.000538\n",
      "      global_mean__OFM_component_24       0.000496\n",
      "                    rbf_bin21__mean       0.000471\n",
      "                    rbf_bin32__mean       0.000449\n",
      "                     rbf_bin20__std       0.000355\n",
      "    global_mean__atomic_number_norm       0.000347\n",
      "     global_std__atomic_radius_norm       0.000339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sand\\AppData\\Local\\Temp\\ipykernel_3756\\3228824978.py:240: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, p = pearsonr(col, y)\n",
      "C:\\Users\\Sand\\AppData\\Local\\Temp\\ipykernel_3756\\3228824978.py:244: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, pr = spearmanr(col, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature_correlations_index.csv\n",
      "Saved importance_by_meaning_correlation.csv\n",
      "\n",
      "Top-10 meanings by max |Pearson r|:\n",
      "                            meaning  abs_pearson\n",
      "      global_mean__OFM_component_13     0.635680\n",
      "        Fe__std__atomic_number_norm     0.589942\n",
      "                          Fe__count     0.564021\n",
      "                global_mean__magmom     0.534105\n",
      "          Fe__std__atomic_mass_norm     0.526959\n",
      "    Fe__std__electron_affinity_norm     0.517485\n",
      "global_mean__electron_affinity_norm     0.432742\n",
      "                   Fe__mean__magmom     0.379303\n",
      "         Fe__mean__atomic_mass_norm     0.379303\n",
      "       Fe__mean__atomic_number_norm     0.379303\n",
      "\n",
      "All results saved to: cgvec_mapping\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell C: Train RF + SHAP + correlation analysis\n",
    "# ----------------------------\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import shap\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "X_path = 'cgvec_mapping/X_cgvecpp.npy'\n",
    "y_path = 'cgvec_mapping/y_cgvecpp.npy'\n",
    "mapping_json = 'cgvec_mapping/mapping_global.json'\n",
    "out_dir = 'cgvec_mapping'\n",
    "rf_file = os.path.join(out_dir, 'rf_cgvecpp.joblib')\n",
    "n_estimators = 200\n",
    "random_state = 42\n",
    "\n",
    "# SHAP settings\n",
    "shap_bg_size = 200    # background size for TreeExplainer\n",
    "shap_eval_size = 500  # number of test samples to compute SHAP for (subset)\n",
    "\n",
    "# ---------- 0. Load data & mapping ----------\n",
    "X = np.load(X_path)\n",
    "y = np.load(y_path)\n",
    "N, D_total = X.shape\n",
    "print(f\"Loaded X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "with open(mapping_json, 'r') as f:\n",
    "    mapping_global = json.load(f)\n",
    "\n",
    "# Build per-index mapping (feature_index -> meaning, block_type)\n",
    "rows = []\n",
    "for m in mapping_global:\n",
    "    start = m['start']; end = m['end']\n",
    "    for fi in range(start, end):\n",
    "        rows.append({'feature_index': fi, 'block_type': m['type'], 'meta': {k:v for k,v in m.items() if k not in ['start','end']}})\n",
    "df_map = pd.DataFrame(rows).sort_values('feature_index').reset_index(drop=True)\n",
    "\n",
    "# We must decode per-index \"meaning\" to human-readable descriptors.\n",
    "# We infer D_node (per-atom) from the first species_block length; D_node = (block_len - 1)/2\n",
    "# Find first species_block to infer D_node\n",
    "species_blocks = [m for m in mapping_global if m['type']=='species_block']\n",
    "if len(species_blocks) == 0:\n",
    "    raise RuntimeError(\"No species_block entries found in mapping_global.json\")\n",
    "first_block = species_blocks[0]\n",
    "block_len = first_block['end'] - first_block['start']\n",
    "D_node = int((block_len - 1) // 2)\n",
    "# We also infer number of RBF bins by counting edge_rbf_bin entries (if present)\n",
    "rbf_bins = len([m for m in mapping_global if m['type']=='edge_rbf_bin'])\n",
    "print(f\"Inferred D_node={D_node}, rbf_bins={rbf_bins}\")\n",
    "\n",
    "# helper to decode atom feature name for a given atom-feature offset (0..D_node-1)\n",
    "def decode_atom_feature_name(offset):\n",
    "    # Your CIFData node layout: [magmom] + OFM(32) + featurizer(4)\n",
    "    # magmom at index 0\n",
    "    if offset == 0:\n",
    "        return 'magmom'\n",
    "    # OFM components 1..32 => offsets 1..32 (if D_node>=33)\n",
    "    ofm_start = 1\n",
    "    ofm_len = 32\n",
    "    if ofm_start <= offset < ofm_start + ofm_len:\n",
    "        return f'OFM_component_{offset - ofm_start}'\n",
    "    # featurizer after OFM\n",
    "    feat_start = ofm_start + ofm_len\n",
    "    fea_names = ['atomic_mass_norm','atomic_number_norm','atomic_radius_norm','electron_affinity_norm']\n",
    "    if offset >= feat_start and offset < feat_start + len(fea_names):\n",
    "        return fea_names[offset - feat_start]\n",
    "    # fallback\n",
    "    return f'node_{offset}'\n",
    "\n",
    "# Now build readable 'meaning' per feature_index using mapping_global metadata\n",
    "meaning_list = []\n",
    "for idx, row in df_map.iterrows():\n",
    "    fi = int(row['feature_index'])\n",
    "    block = row['block_type']\n",
    "    meta = row['meta']\n",
    "    start = next(m['start'] for m in mapping_global if m['start'] <= fi < m['end'])\n",
    "    block_entry = next(m for m in mapping_global if m['start'] == start)\n",
    "    offset = fi - block_entry['start']\n",
    "    meaning = None\n",
    "    if block == 'species_block':\n",
    "        species = block_entry['species']\n",
    "        if offset == 0:\n",
    "            meaning = f\"{species}__count\"\n",
    "        elif 1 <= offset <= D_node:\n",
    "            atom_pos = offset - 1\n",
    "            meaning = f\"{species}__mean__{decode_atom_feature_name(atom_pos)}\"\n",
    "        else:\n",
    "            atom_pos = offset - 1 - D_node\n",
    "            meaning = f\"{species}__std__{decode_atom_feature_name(atom_pos)}\"\n",
    "    elif block == 'edge_rbf_bin':\n",
    "        rbf_idx = int(block_entry.get('rbf_idx', offset//2 if rbf_bins>0 else 0))\n",
    "        if offset % 2 == 0:\n",
    "            meaning = f\"rbf_bin{rbf_idx}__mean\"\n",
    "        else:\n",
    "            meaning = f\"rbf_bin{rbf_idx}__std\"\n",
    "    elif block == 'global_atom_mean':\n",
    "        atom_pos = offset\n",
    "        meaning = f\"global_mean__{decode_atom_feature_name(atom_pos)}\"\n",
    "    elif block == 'global_atom_std':\n",
    "        atom_pos = offset\n",
    "        meaning = f\"global_std__{decode_atom_feature_name(atom_pos)}\"\n",
    "    else:\n",
    "        meaning = f\"{block}__offset{offset}\"\n",
    "    meaning_list.append(meaning)\n",
    "\n",
    "df_map['meaning'] = meaning_list\n",
    "# save mapping table per-index\n",
    "df_map.to_csv(os.path.join(out_dir, 'feature_index_meaning.csv'), index=False)\n",
    "print(\"Saved feature_index_meaning.csv\")\n",
    "\n",
    "# ---------- 1. Stratified 60/20/20 split like original ----------\n",
    "def make_safe_stratification_bins(y, min_per_bin=30, max_bins=10):\n",
    "    for n_bins in range(max_bins, 1, -1):\n",
    "        try:\n",
    "            disc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "            y_binned = disc.fit_transform(y.reshape(-1,1)).astype(int).ravel()\n",
    "            _, counts = np.unique(y_binned, return_counts=True)\n",
    "            if counts.min() >= min_per_bin:\n",
    "                print(f\"Using {n_bins} quantile bins for stratified split (min bin size {counts.min()})\")\n",
    "                return y_binned\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(\"Could not bin targets for stratified split\")\n",
    "\n",
    "y_binned = make_safe_stratification_bins(y, min_per_bin=30, max_bins=10)\n",
    "idx = np.arange(N)\n",
    "train_idx, temp_idx, _, temp_bins = train_test_split(idx, y_binned, test_size=0.4, stratify=y_binned, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=temp_bins, random_state=random_state)\n",
    "\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
    "print(\"Split sizes -> train:\", len(X_train), \"val:\", len(X_val), \"test:\", len(X_test))\n",
    "\n",
    "# ---------- 2. Train RandomForest (original-style) ----------\n",
    "rf = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "joblib.dump(rf, rf_file)\n",
    "print(f\"RF trained in {t1-t0:.2f}s and saved to {rf_file}\")\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(model, Xs, ys):\n",
    "    preds = model.predict(Xs)\n",
    "    return {\n",
    "        'R2': r2_score(ys, preds),\n",
    "        'MAE': mean_absolute_error(ys, preds),\n",
    "        'RMSE': mean_squared_error(ys, preds, squared=False),\n",
    "        'CC': pearsonr(ys, preds)[0]\n",
    "    }\n",
    "\n",
    "val_metrics = evaluate(rf, X_val, y_val)\n",
    "test_metrics = evaluate(rf, X_test, y_test)\n",
    "print(\"Validation metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "\n",
    "# ---------- 3. Impurity-based importances (per-index) ----------\n",
    "fi = rf.feature_importances_\n",
    "df_imp = df_map[['feature_index','meaning','block_type']].copy()\n",
    "df_imp['importance_impurity'] = fi[df_imp['feature_index'].values]\n",
    "df_imp.to_csv(os.path.join(out_dir, 'feature_importances_vector_index.csv'), index=False)\n",
    "print(\"Saved feature_importances_vector_index.csv\")\n",
    "\n",
    "# Aggregate impurity importance by physical meaning (sum across identical meanings)\n",
    "agg_imp = df_imp.groupby('meaning', as_index=False)['importance_impurity'].sum().sort_values('importance_impurity', ascending=False)\n",
    "agg_imp.to_csv(os.path.join(out_dir, 'importance_by_meaning_impurity.csv'), index=False)\n",
    "print(\"Saved importance_by_meaning_impurity.csv\")\n",
    "\n",
    "# ---------- 4. SHAP (TreeExplainer) -> mean absolute SHAP per index -> aggregate by meaning ----------\n",
    "# Create background sample from TRAIN for explainer\n",
    "# ===== REPLACEMENT SHAP BLOCK (robust to additivity errors) =====\n",
    "import warnings\n",
    "\n",
    "# SHAP settings already defined earlier: shap_bg_size, shap_eval_size\n",
    "bg_idx = np.random.RandomState(random_state).choice(len(X_train), size=min(shap_bg_size, len(X_train)), replace=False)\n",
    "bg = X_train[bg_idx]\n",
    "\n",
    "# Preferred: interventional TreeExplainer (more robust for correlated features)\n",
    "# If this still raises an additivity error we fall back to check_additivity=False.\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(rf, data=bg, feature_perturbation=\"interventional\")\n",
    "    # compute shap values (TreeExplainer returns array-like)\n",
    "    X_eval = X_test[np.random.RandomState(random_state+1).choice(len(X_test), size=min(shap_eval_size, len(X_test)), replace=False)]\n",
    "    print(\"Computing SHAP values with TreeExplainer(feature_perturbation='interventional') on eval subset (size {})...\".format(X_eval.shape[0]))\n",
    "    shap_vals = explainer.shap_values(X_eval)   # may be numpy array or list\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Interventional explainer failed with: {e}. Falling back to check_additivity=False with default perturbation.\")\n",
    "    explainer = shap.TreeExplainer(rf, data=bg)  # default (tree path) explainer\n",
    "    X_eval = X_test[np.random.RandomState(random_state+1).choice(len(X_test), size=min(shap_eval_size, len(X_test)), replace=False)]\n",
    "    try:\n",
    "        shap_vals = explainer.shap_values(X_eval, check_additivity=False)\n",
    "    except Exception as e2:\n",
    "        # Last resort: compute SHAP per-sample in a loop with check_additivity=False to avoid large memory peaks\n",
    "        warnings.warn(f\"Batch SHAP failed with: {e2}. Falling back to per-sample SHAP computation (slower).\")\n",
    "        shap_list = []\n",
    "        for xi in X_eval:\n",
    "            sv = explainer.shap_values(xi.reshape(1,-1), check_additivity=False)\n",
    "            shap_list.append(np.array(sv).reshape(-1))\n",
    "        shap_vals = np.vstack(shap_list)\n",
    "\n",
    "# shap_vals should now be a (n_eval, n_features) numpy array for regression\n",
    "shap_arr = np.array(shap_vals)  # ensure np.array type\n",
    "if shap_arr.ndim == 3:\n",
    "    # shap_values can sometimes be (n_eval, n_features, 1) for regression with some versions\n",
    "    shap_arr = shap_arr.reshape(shap_arr.shape[0], shap_arr.shape[1])\n",
    "\n",
    "# Aggregate mean absolute SHAP per feature index\n",
    "mean_abs_shap = np.mean(np.abs(shap_arr), axis=0)\n",
    "\n",
    "# Save per-index SHAP to CSV (same format as before)\n",
    "df_shap = df_map[['feature_index','meaning','block_type']].copy()\n",
    "df_shap['mean_abs_shap'] = mean_abs_shap[df_shap['feature_index'].values]\n",
    "df_shap.to_csv(os.path.join(out_dir, 'feature_importances_with_shap.csv'), index=False)\n",
    "print(\"Saved feature_importances_with_shap.csv (robust SHAP)\")\n",
    "\n",
    "# Aggregate SHAP by meaning and save\n",
    "agg_shap = df_shap.groupby('meaning', as_index=False)['mean_abs_shap'].sum().sort_values('mean_abs_shap', ascending=False)\n",
    "agg_shap.to_csv(os.path.join(out_dir, 'importance_by_meaning_shap.csv'), index=False)\n",
    "print(\"Saved importance_by_meaning_shap.csv (robust SHAP)\")\n",
    "\n",
    "print(\"\\nTop-20 SHAP-aggregated meanings (robust):\")\n",
    "print(agg_shap.head(20).to_string(index=False))\n",
    "# ===== end replacement SHAP block =====\n",
    "\n",
    "# ---------- 5. Correlation analysis (per-index) ----------\n",
    "pearson_r = []\n",
    "pearson_p = []\n",
    "spearman_r = []\n",
    "spearman_p = []\n",
    "for fi_idx in df_map['feature_index'].values:\n",
    "    col = X[:,fi_idx]\n",
    "    try:\n",
    "        r, p = pearsonr(col, y)\n",
    "    except Exception:\n",
    "        r, p = np.nan, np.nan\n",
    "    try:\n",
    "        rho, pr = spearmanr(col, y)\n",
    "    except Exception:\n",
    "        rho, pr = np.nan, np.nan\n",
    "    pearson_r.append(r); pearson_p.append(p)\n",
    "    spearman_r.append(rho); spearman_p.append(pr)\n",
    "\n",
    "df_corr = df_map[['feature_index','meaning','block_type']].copy()\n",
    "df_corr['pearson_r'] = pearson_r\n",
    "df_corr['pearson_p'] = pearson_p\n",
    "df_corr['spearman_rho'] = spearman_r\n",
    "df_corr['spearman_p'] = spearman_p\n",
    "df_corr.to_csv(os.path.join(out_dir, 'feature_correlations_index.csv'), index=False)\n",
    "print(\"Saved feature_correlations_index.csv\")\n",
    "\n",
    "# Also aggregate correlations to a per-meaning measure by taking max absolute Pearson among indices sharing meaning (conservative)\n",
    "agg_corr = df_corr.copy()\n",
    "agg_corr['abs_pearson'] = np.abs(agg_corr['pearson_r'])\n",
    "agg_corr_summary = agg_corr.groupby('meaning', as_index=False)['abs_pearson'].max().sort_values('abs_pearson', ascending=False)\n",
    "agg_corr_summary.to_csv(os.path.join(out_dir, 'importance_by_meaning_correlation.csv'), index=False)\n",
    "print(\"Saved importance_by_meaning_correlation.csv\")\n",
    "\n",
    "# ---------- 6. Print top-10 correlations (meaning-level) ----------\n",
    "print(\"\\nTop-10 meanings by max |Pearson r|:\")\n",
    "print(agg_corr_summary.head(10).to_string(index=False))\n",
    "\n",
    "# Done\n",
    "print(\"\\nAll results saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68c97c",
   "metadata": {},
   "source": [
    "Feature Analysis plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f227dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files checked. Proceeding to load available data...\n",
      "Loaded X,y shapes: (3000, 6386) (3000,)\n",
      "Using reproducible splits. Test size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sand\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig1 saved. Test metrics: R2=0.7860, CC=0.8867, MAE=0.01116, RMSE=0.01703\n",
      "Fig2 saved: top20_meaning_shap\n",
      "Fig3 saved: top20_meaning_impurity\n",
      "Fig4 saved: descriptor_corr_heatmap\n",
      "Fig5 saved (bar fallback): shap_top30_bar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sand\\AppData\\Local\\Temp\\ipykernel_3756\\3786483809.py:225: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='species', y='mean_abs_shap', data=df_top, palette='muted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig6 saved: cgvec_mapping\\figures\\shap_by_species_top10.png\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Plotting cell (run after Cell C success)\n",
    "# Generates paper-ready figures and saves them to cgvec_mapping/figures/\n",
    "# ----------------------------\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import joblib\n",
    "\n",
    "# --------- Files and folders ----------\n",
    "out_dir = 'cgvec_mapping'\n",
    "fig_dir = os.path.join(out_dir, 'figures')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# Required files (created by previous cells)\n",
    "files = {\n",
    "    'X': os.path.join(out_dir, 'X_cgvecpp.npy'),\n",
    "    'y': os.path.join(out_dir, 'y_cgvecpp.npy'),\n",
    "    'map': os.path.join(out_dir, 'feature_index_meaning.csv'),\n",
    "    'impurity': os.path.join(out_dir, 'feature_importances_vector_index.csv'),\n",
    "    'shap_index': os.path.join(out_dir, 'feature_importances_with_shap.csv'),\n",
    "    'agg_shap': os.path.join(out_dir, 'importance_by_meaning_shap.csv'),\n",
    "    'agg_impurity': os.path.join(out_dir, 'importance_by_meaning_impurity.csv'),\n",
    "    'corr_index': os.path.join(out_dir, 'feature_correlations_index.csv'),\n",
    "    'rf_model': os.path.join(out_dir, 'rf_cgvecpp.joblib')\n",
    "}\n",
    "\n",
    "# Quick existence check\n",
    "for k,v in files.items():\n",
    "    if not os.path.exists(v):\n",
    "        print(f\"WARNING: missing file {k}: {v}  (some plots may be skipped)\")\n",
    "print(\"Files checked. Proceeding to load available data...\")\n",
    "\n",
    "# Load arrays\n",
    "X = np.load(files['X'])\n",
    "y = np.load(files['y'])\n",
    "N, D = X.shape\n",
    "print(\"Loaded X,y shapes:\", X.shape, y.shape)\n",
    "\n",
    "# Load mapping, importance, shap, correlations (if present)\n",
    "df_map = pd.read_csv(files['map']) if os.path.exists(files['map']) else None\n",
    "df_imp_idx = pd.read_csv(files['impurity']) if os.path.exists(files['impurity']) else None\n",
    "df_shap_idx = pd.read_csv(files['shap_index']) if os.path.exists(files['shap_index']) else None\n",
    "df_agg_shap = pd.read_csv(files['agg_shap']) if os.path.exists(files['agg_shap']) else None\n",
    "df_agg_imp = pd.read_csv(files['agg_impurity']) if os.path.exists(files['agg_impurity']) else None\n",
    "df_corr = pd.read_csv(files['corr_index']) if os.path.exists(files['corr_index']) else None\n",
    "rf = joblib.load(files['rf_model']) if os.path.exists(files['rf_model']) else None\n",
    "\n",
    "# Load test split predictions: if you trained earlier and still have splits in memory it's better.\n",
    "# If not available, we will recompute predictions by splitting dataset again reproducibly.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def make_safe_stratification_bins(y, min_per_bin=30, max_bins=10):\n",
    "    for n_bins in range(max_bins, 1, -1):\n",
    "        try:\n",
    "            disc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "            y_binned = disc.fit_transform(y.reshape(-1,1)).astype(int).ravel()\n",
    "            _, counts = np.unique(y_binned, return_counts=True)\n",
    "            if counts.min() >= min_per_bin:\n",
    "                return y_binned\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(\"Could not bin targets for stratified split\")\n",
    "\n",
    "y_binned = make_safe_stratification_bins(y, min_per_bin=30, max_bins=10)\n",
    "idx = np.arange(N)\n",
    "train_idx, temp_idx, _, temp_bins = train_test_split(idx, y_binned, test_size=0.4, stratify=y_binned, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=temp_bins, random_state=42)\n",
    "\n",
    "X_test = X[test_idx]; y_test = y[test_idx]\n",
    "print(\"Using reproducible splits. Test size:\", len(X_test))\n",
    "\n",
    "# Predictions\n",
    "if rf is not None:\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "else:\n",
    "    raise RuntimeError(\"RF model not found; cannot produce True vs Predicted plot.\")\n",
    "\n",
    "# ---------- Fig 1: True vs Predicted (Test) ----------\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k', linewidth=0.2)\n",
    "mn = min(y_test.min(), y_pred_test.min()); mx = max(y_test.max(), y_pred_test.max())\n",
    "plt.plot([mn, mx], [mn, mx], 'r--', lw=1.5)\n",
    "plt.xlabel('True Magnetization')\n",
    "plt.ylabel('Predicted Magnetization')\n",
    "plt.title('True vs Predicted Magnetization (Test)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(fig_dir, 'true_vs_pred_test.png'), dpi=300)\n",
    "plt.savefig(os.path.join(fig_dir, 'true_vs_pred_test.pdf'))\n",
    "plt.close()\n",
    "# numeric metrics for caption\n",
    "r2 = rf.score(X_test, y_test)\n",
    "cc = pearsonr(y_test, y_pred_test)[0]\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "rmse = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "print(\"Fig1 saved. Test metrics: R2={:.4f}, CC={:.4f}, MAE={:.5f}, RMSE={:.5f}\".format(r2, cc, mae, rmse))\n",
    "\n",
    "# ---------- Helper: select and prepare top-N by SHAP or impurity ----------\n",
    "def top_meaning_df(df_agg, topn=20):\n",
    "    df = df_agg.copy()\n",
    "    df = df.sort_values(df.columns[1], ascending=False).head(topn)  # second column assumed to be importance\n",
    "    # rename cols for convenience\n",
    "    df.columns = ['meaning','importance']\n",
    "    return df\n",
    "\n",
    "# ---------- Fig 2: Top-20 meanings by SHAP (aggregated) ----------\n",
    "if df_agg_shap is not None:\n",
    "    df_top_shap = top_meaning_df(df_agg_shap, topn=20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(np.arange(len(df_top_shap)), df_top_shap['importance'][::-1])\n",
    "    plt.yticks(np.arange(len(df_top_shap)), df_top_shap['meaning'][::-1], fontsize=8)\n",
    "    plt.xlabel('Aggregated mean(|SHAP|)')\n",
    "    plt.title('Top-20 descriptors by SHAP importance (aggregated)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, 'top20_meaning_shap.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(fig_dir, 'top20_meaning_shap.pdf'))\n",
    "    plt.close()\n",
    "    print(\"Fig2 saved: top20_meaning_shap\")\n",
    "else:\n",
    "    print(\"SHAP aggregated CSV not found; skipping Fig2 (top SHAP).\")\n",
    "\n",
    "# ---------- Fig 3: Top-20 meanings by impurity importance ----------\n",
    "if df_agg_imp is not None:\n",
    "    df_top_imp = top_meaning_df(df_agg_imp, topn=20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(np.arange(len(df_top_imp)), df_top_imp['importance'][::-1])\n",
    "    plt.yticks(np.arange(len(df_top_imp)), df_top_imp['meaning'][::-1], fontsize=8)\n",
    "    plt.xlabel('Aggregated impurity importance')\n",
    "    plt.title('Top-20 descriptors by impurity importance (aggregated)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, 'top20_meaning_impurity.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(fig_dir, 'top20_meaning_impurity.pdf'))\n",
    "    plt.close()\n",
    "    print(\"Fig3 saved: top20_meaning_impurity\")\n",
    "else:\n",
    "    print(\"Impurity aggregated CSV not found; skipping Fig3.\")\n",
    "\n",
    "# ---------- Fig 4: Correlation heatmap for selected descriptors (OFM, magmom, rbf bins) ----------\n",
    "if df_corr is not None:\n",
    "    # choose descriptors to include: OFM components and magmom and top rbf bins by abs Pearson\n",
    "    # find all meanings that include OFM_component or magmom or rbf_bin\n",
    "    sel_mask = df_corr['meaning'].str.contains('OFM_component|__magmom|rbf_bin')\n",
    "    df_sel = df_corr[sel_mask].copy()\n",
    "    # pick top K by abs Pearson\n",
    "    df_sel['abs_pearson'] = df_sel['pearson_r'].abs()\n",
    "    df_sel_top = df_sel.sort_values('abs_pearson', ascending=False).head(30)\n",
    "    # build matrix of values across samples for these meanings (extract column indices of the meaning)\n",
    "    sel_indices = df_sel_top['feature_index'].values.astype(int).tolist()\n",
    "    data_mat = X[:, sel_indices]\n",
    "    # compute correlation matrix among selected descriptors (not to be confused with corr with target)\n",
    "    corrmat = np.corrcoef(data_mat.T)\n",
    "    names = df_sel_top['meaning'].values\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corrmat, xticklabels=names, yticklabels=names, vmax=1.0, vmin=-1.0, cmap='coolwarm', center=0)\n",
    "    plt.title('Descriptor correlation matrix (top 30 OFM/magmom/rbf descriptors)')\n",
    "    plt.xticks(rotation=90, fontsize=7)\n",
    "    plt.yticks(rotation=0, fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, 'descriptor_corr_heatmap.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(fig_dir, 'descriptor_corr_heatmap.pdf'))\n",
    "    plt.close()\n",
    "    print(\"Fig4 saved: descriptor_corr_heatmap\")\n",
    "else:\n",
    "    print(\"Correlation CSV not found; skipping Fig4.\")\n",
    "\n",
    "# ---------- Fig 5: SHAP beeswarm for top per-index features (if shap per-index array exists) ----------\n",
    "# Try to load raw shap values used earlier from 'explainer' step if saved; else skip.\n",
    "shap_idx_path = os.path.join(out_dir, 'shap_values_eval.npy')\n",
    "if os.path.exists(shap_idx_path) and df_shap_idx is not None:\n",
    "    shap_vals_full = np.load(shap_idx_path)  # shape (n_eval, n_features)\n",
    "    # get top per-index by mean_abs_shap\n",
    "    top_idx = df_shap_idx.sort_values('mean_abs_shap', ascending=False).head(30)['feature_index'].values.astype(int)\n",
    "    import shap as _shap\n",
    "    plt.figure(figsize=(8,6))\n",
    "    _shap.summary_plot(shap_vals_full[:, top_idx], X_test[:, top_idx], plot_type='dot', show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, 'shap_beeswarm_top30.png'), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Fig5 saved: shap_beeswarm_top30\")\n",
    "else:\n",
    "    # fallback: if df_shap_idx present but raw shap array not saved, produce bar chart of per-index SHAP top30 instead\n",
    "    if df_shap_idx is not None:\n",
    "        df_top30_idx = df_shap_idx.sort_values('mean_abs_shap', ascending=False).head(30)\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.barh(range(len(df_top30_idx)), df_top30_idx['mean_abs_shap'][::-1])\n",
    "        plt.yticks(range(len(df_top30_idx)), df_top30_idx['meaning'][::-1], fontsize=7)\n",
    "        plt.xlabel('mean(|SHAP|)')\n",
    "        plt.title('Top-30 per-index features by mean(|SHAP|)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fig_dir, 'shap_top30_bar.png'), dpi=300)\n",
    "        plt.close()\n",
    "        print(\"Fig5 saved (bar fallback): shap_top30_bar\")\n",
    "    else:\n",
    "        print(\"SHAP per-index CSV not found; skipping Fig5.\")\n",
    "\n",
    "# ---------- Fig 6: Per-species aggregated SHAP importance (if multiple species present) ----------\n",
    "# ---- Replacement Fig6: show TOP-10 species only (clean, readable) ----\n",
    "if df_agg_shap is not None:\n",
    "    df = df_agg_shap.copy()\n",
    "\n",
    "    # same species-extraction heuristic as before\n",
    "    def species_from_meaning(s):\n",
    "        p = s.split('__')[0]\n",
    "        if p in ('global_mean', 'global_std') or p.startswith('rbf_bin'):\n",
    "            return 'GLOBAL'\n",
    "        if len(p) <= 3 and p.isalpha():\n",
    "            return p\n",
    "        return 'OTHER'\n",
    "\n",
    "    df['species'] = df['meaning'].apply(species_from_meaning)\n",
    "    species_group = df.groupby('species')['mean_abs_shap'].sum().reset_index()\n",
    "    species_group = species_group.sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "    topN = 10\n",
    "    df_top = species_group.head(topN)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    import seaborn as sns\n",
    "    sns.barplot(x='species', y='mean_abs_shap', data=df_top, palette='muted')\n",
    "    plt.xlabel('Species or GLOBAL', fontsize=11)\n",
    "    plt.ylabel('Sum mean(|SHAP|)', fontsize=11)\n",
    "    plt.title('Aggregated SHAP by species (top 10)', fontsize=13)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(fig_dir, 'shap_by_species_top10.png')\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Fig6 saved:\", out_path)\n",
    "else:\n",
    "    print(\"Aggregated SHAP CSV missing; skipping Fig6.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6d878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clean Top-20 SHAP and impurity plots saved to: cgvec_mapping\\figures\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Code 5 (REVISED):\n",
    "# Clean, paper-ready plots for Top-20 CGVEC descriptors\n",
    "# - Expanded x-axis (2Ã— max importance)\n",
    "# - Structured y-axis labels: FeatureGroup[dim] â†’ Findex / Ftotal\n",
    "# ----------------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Paths ----------\n",
    "out_dir = 'cgvec_mapping'\n",
    "fig_dir = os.path.join(out_dir, 'figures')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ---------- Load data ----------\n",
    "df_shap = pd.read_csv(os.path.join(out_dir, 'importance_by_meaning_shap.csv'))\n",
    "df_imp  = pd.read_csv(os.path.join(out_dir, 'importance_by_meaning_impurity.csv'))\n",
    "df_map  = pd.read_csv(os.path.join(out_dir, 'feature_index_meaning.csv'))\n",
    "\n",
    "D_total = df_map['feature_index'].max() + 1  # total vector length\n",
    "\n",
    "# ---------- Helper: clean feature label ----------\n",
    "def format_label(meaning):\n",
    "    \"\"\"\n",
    "    Converts:\n",
    "    global_mean OFM[13] (idx 6326)\n",
    "    â†’ OFM[13] (F6326 / 6386)\n",
    "    \"\"\"\n",
    "    row = df_map[df_map['meaning'] == meaning]\n",
    "    if len(row) == 0:\n",
    "        return meaning\n",
    "\n",
    "    idx = int(row.iloc[0]['feature_index'])\n",
    "\n",
    "    if 'OFM_component_' in meaning:\n",
    "        k = meaning.split('OFM_component_')[-1]\n",
    "        return f'OFM[{k}]  (F{idx}/{D_total})'\n",
    "\n",
    "    if '__magmom' in meaning:\n",
    "        return f'magmom  (F{idx}/{D_total})'\n",
    "\n",
    "    if 'rbf_bin' in meaning:\n",
    "        bin_id = meaning.split('rbf_bin')[-1].split('__')[0]\n",
    "        stat = 'mean' if 'mean' in meaning else 'std'\n",
    "        return f'RBF[{bin_id}] {stat}  (F{idx}/{D_total})'\n",
    "\n",
    "    if 'atomic_' in meaning:\n",
    "        name = meaning.split('__')[-1].replace('_norm', '')\n",
    "        return f'{name}  (F{idx}/{D_total})'\n",
    "\n",
    "    return f'{meaning}  (F{idx}/{D_total})'\n",
    "\n",
    "\n",
    "# ---------- Generic plotting function ----------\n",
    "def plot_top20(df, value_col, title, xlabel, filename):\n",
    "    df_top = df.sort_values(value_col, ascending=False).head(20).copy()\n",
    "    df_top['label'] = df_top['meaning'].apply(format_label)\n",
    "\n",
    "    values = df_top[value_col].values\n",
    "    labels = df_top['label'].values\n",
    "\n",
    "    xmax = values.max() * 2.0  # ðŸ”‘ double x-axis range\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.barh(range(len(values)), values[::-1])\n",
    "    plt.yticks(range(len(labels)), labels[::-1], fontsize=9)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(title)\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(fig_dir, filename + '.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(fig_dir, filename + '.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ---------- Plot 1: SHAP ----------\n",
    "plot_top20(\n",
    "    df_shap,\n",
    "    value_col='mean_abs_shap',\n",
    "    title='Top-20 CGVEC descriptors by SHAP importance',\n",
    "    xlabel='Aggregated mean(|SHAP|)',\n",
    "    filename='top20_cgvec_shap_clean'\n",
    ")\n",
    "\n",
    "# ---------- Plot 2: Impurity ----------\n",
    "plot_top20(\n",
    "    df_imp,\n",
    "    value_col='importance_impurity',\n",
    "    title='Top-20 CGVEC descriptors by impurity importance',\n",
    "    xlabel='Aggregated impurity importance',\n",
    "    filename='top20_cgvec_impurity_clean'\n",
    ")\n",
    "\n",
    "print(\"âœ… Clean Top-20 SHAP and impurity plots saved to:\", fig_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456887d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cgvec_mapping\\top20_cgvec_shap_clean.pdf\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Top-20 CGVEC SHAP plot (aggregated)\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Paths --------\n",
    "base_dir = \"cgvec_mapping\"\n",
    "shap_file = os.path.join(base_dir, \"importance_by_meaning_shap.csv\")\n",
    "map_file  = os.path.join(base_dir, \"feature_index_meaning.csv\")\n",
    "out_fig   = os.path.join(base_dir, \"top20_cgvec_shap_clean.pdf\")\n",
    "\n",
    "# -------- Load data --------\n",
    "df_shap = pd.read_csv(shap_file)\n",
    "df_map  = pd.read_csv(map_file)\n",
    "\n",
    "# -------- Take top-20 unique meanings --------\n",
    "df_top = (\n",
    "    df_shap\n",
    "    .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "    .head(20)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -------- Map each meaning to a representative F-index --------\n",
    "# (first occurrence in the vector)\n",
    "meaning_to_f = (\n",
    "    df_map\n",
    "    .groupby(\"meaning\")[\"feature_index\"]\n",
    "    .min()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df_top[\"F_label\"] = df_top[\"meaning\"].map(\n",
    "    lambda m: f\"F{meaning_to_f[m]:03d}\"\n",
    ")\n",
    "\n",
    "# -------- Short, SAFE physics-aware names --------\n",
    "def short_name(m):\n",
    "    if \"OFM_component_12\" in m:\n",
    "        return \"dâµ valence occupancy\"\n",
    "    if \"OFM_component_13\" in m:\n",
    "        return \"dâ¶ valence occupancy\"\n",
    "    if \"OFM_component_14\" in m:\n",
    "        return \"dâ· valence occupancy\"\n",
    "    if \"OFM_component_24\" in m:\n",
    "        return \"fâ· valence occupancy\"\n",
    "    if \"magmom\" in m and \"mean\" in m:\n",
    "        return \"mean atomic spin moment\"\n",
    "    if \"magmom\" in m and \"std\" in m:\n",
    "        return \"variation of atomic spin moments\"\n",
    "    if \"rbf_bin\" in m and \"__mean\" in m:\n",
    "        k = int(m.split(\"rbf_bin\")[1].split(\"__\")[0])\n",
    "        return f\"mean neighbor contribution (~{k*0.1:.1f} Ã…)\"\n",
    "    if \"rbf_bin\" in m and \"__std\" in m:\n",
    "        k = int(m.split(\"rbf_bin\")[1].split(\"__\")[0])\n",
    "        return f\"variation of neighbor contribution (~{k*0.1:.1f} Ã…)\"\n",
    "    if \"__count\" in m:\n",
    "        return \"species atom count\"\n",
    "    if \"atomic_number_norm\" in m and \"mean\" in m:\n",
    "        return \"mean atomic number\"\n",
    "    if \"atomic_number_norm\" in m and \"std\" in m:\n",
    "        return \"variation in atomic number\"\n",
    "\n",
    "    if \"atomic_radius_norm\" in m and \"mean\" in m:\n",
    "        return \"mean atomic radius\"\n",
    "    if \"atomic_radius_norm\" in m and \"std\" in m:\n",
    "         return \"variation in atomic radius\"\n",
    "\n",
    "    if \"electron_affinity_norm\" in m and \"mean\" in m:\n",
    "        return \"mean electron affinity\"\n",
    "    if \"electron_affinity_norm\" in m and \"std\" in m:\n",
    "        return \"variation in electron affinity\"\n",
    "\n",
    "\n",
    "df_top[\"short_name\"] = df_top[\"meaning\"].apply(short_name)\n",
    "\n",
    "# -------- Plot --------\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "y_pos = np.arange(len(df_top))\n",
    "\n",
    "plt.barh(\n",
    "    y_pos,\n",
    "    df_top[\"mean_abs_shap\"],\n",
    "    color=\"#4C72B0\"\n",
    ")\n",
    "\n",
    "# Y-axis labels: F-indices only\n",
    "plt.yticks(y_pos, df_top[\"F_label\"], fontsize=10)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# X-axis\n",
    "plt.xlabel(\"Average feature contribution (SHAP)\", fontsize=12)\n",
    "plt.xlim(0.0, 0.023)\n",
    "\n",
    "# Add short feature names ABOVE bars\n",
    "for i, (val, name) in enumerate(zip(df_top[\"mean_abs_shap\"], df_top[\"short_name\"])):\n",
    "    plt.text(\n",
    "        val + 0.0004,\n",
    "        i,\n",
    "        name,\n",
    "        va=\"center\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.title(\"Top 20 CGVEC Features by SHAP Importance\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(out_fig)\n",
    "plt.savefig(out_fig.replace(\".pdf\", \".png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved:\", out_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f3d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
